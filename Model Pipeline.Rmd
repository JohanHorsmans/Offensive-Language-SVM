---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(pacman)
p_load(readr, tidyverse,rsample,recipes, textrecipes, parsnip, yardstick,workflows, discrim,kernlab)
p_load(tidyverse, stringr, tm, ggplot2, GGally, e1071, caret,stopwords, stringi, tm, SnowballC,stringr,fastmatch)
library("parsnip")

loading_data <- function(path) {
  read_delim(path, "\t", escape_double = FALSE, trim_ws = TRUE)
}

corpus <- loading_data("offenseval-da-training-v1.tsv") %>% 
          mutate(Id = id,label = factor(subtask_a),text=tweet) %>% 
          na.omit()
testing<-loading_data("offenseval-da-test-v1.tsv") %>% 
          mutate(Id = id,label = factor(subtask_a),text=tweet) %>% 
          na.omit()


###CLEANING####

# Remove stopwords
testing$text<-tolower(testing$text) 
corpus$text<-tolower(corpus$text) 

stopwords_regex = paste(stopwords("da",source= "snowball"), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
corpus$text = stringr::str_replace_all(corpus$text, stopwords_regex, '')
# remove numbers
corpus$text <-  removeNumbers(corpus$text)
# Stem words
corpus$text <-  wordStem(corpus$text, language = "danish")
#repeat for test data
stopwords_regex = paste(stopwords("da",source= "snowball"), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
testing$text = stringr::str_replace_all(testing$text, stopwords_regex, '')
# remove numbers
testing$text <-  removeNumbers(testing$text)
# Stem words
testing$text <-  wordStem(testing$text, language = "danish")
# remove punctuation
testing$text<-removePunctuation(testing$text)
corpus$text<-removePunctuation(corpus$text)


###CLEANING###


corpus<-corpus[,4:6]

testing<-testing[,4:6]

training_set <- corpus
test_set <- testing
#


# General toxen recipe
text_recipe <- recipe(label ~ ., data = training_set) %>% 
  update_role(Id, new_role = "ID") %>% 
  step_tokenize(text, engine = "spacyr", token = "words") %>%
 ## step_stopwords(text) %>% 
  step_lemma(text) %>%
  step_tokenfilter(text, max_tokens = 100) %>%
  step_tfidf(text)
#

# NGRAM recipie
text_recipe <- recipe(label ~ ., data = training_set) %>% 
  update_role(Id, new_role = "ID") %>% 
  step_tokenize(text) %>%
  step_ngram(text, num_tokens = 5) %>%
 ## step_stopwords(text) %>% 
  ##step_lemma(text) %>%
  step_tokenfilter(text, max_tokens = 100) %>%
  step_tfidf(text)

#TRI GRAM:
#rec <- recipe(~ text, data = abc_tibble) %>%
 # step_tokenize(text) %>%
  #step_ngram(text, num_tokens = 3) %>%
  #step_tokenfilter(text) %>%
  #step_tf(text)

```

```{r}
text_model_log_spec <- logistic_reg() %>% set_engine("glm") %>% set_mode("classification")
text_model_NB_spec <- naive_Bayes() %>% set_engine("naivebayes") %>% set_mode("classification")
text_model_svm_spec <- svm_poly("classification") %>% set_engine("kernlab")
```

```{r}
text_model_log_wf <- workflows::workflow() %>% add_recipe(text_recipe) %>% add_model(text_model_log_spec)
text_model_NB_wf <- workflows::workflow() %>% add_recipe(text_recipe) %>% add_model(text_model_NB_spec)
text_model_svm_wf <- workflows::workflow() %>% add_recipe(text_recipe) %>% add_model(text_model_svm_spec)

```

```{r}
fit_log_model <- fit(text_model_log_wf, training_set)
fit_NB_model <- fit(text_model_NB_wf, training_set)
fit_svm_model <- fit(text_model_svm_wf, training_set)

```

```{r}
predictions_log <- predict(fit_log_model, test_set)
predictions_log$raw_log <- predict(fit_log_model, test_set,type="prob")

predictions_NB <- predict(fit_NB_model, test_set,type="class")
predictions_NB$raw_NB <- predict(fit_NB_model, test_set,type="prob")

predictions_SVM <- stats::predict(fit_svm_model, test_set,type="class")
predictions_SVM$raw_svm <- stats::predict(fit_svm_model, test_set,type="prob")



#testing$predictions<-predictions_NB


#Evaluate
bind_cols(test_set,predictions_log) %>% conf_mat(label, .pred_class) 
bind_cols(test_set,predictions_log) %>% accuracy(truth = label, estimate = .pred_class)

bind_cols(test_set,predictions_NB) %>% conf_mat(label, .pred_class) 
bind_cols(test_set,predictions_NB) %>% accuracy(truth = label, estimate = .pred_class)

bind_cols(test_set,predictions_SVM) %>% conf_mat(label, .pred_class) 
bind_cols(test_set,predictions_SVM) %>% accuracy(truth = label, estimate = .pred_class)


```

Ensemble averaging pipeline:
```{r}
#Load BERT data:
Bert_results<-read_csv("OG_BERT_RESULTS.csv")
#testing$BERT_not_prob<-Bert_results$`0`
#testing$BERT_off_prob<-Bert_results$`1`

#Create ensemble with raw probs of other models:
Ensemble_probabilities<-predictions_SVM$raw_svm
Ensemble_probabilities$SVM<-predictions_SVM$raw_svm
Ensemble_probabilities<-Ensemble_probabilities[,3]
Ensemble_probabilities$NB<-predictions_NB$raw_NB
Ensemble_probabilities$log<-predictions_log$raw_log


#Define empty columns in testing set
Ensemble_probabilities$ensemble_plus_bert_probs_off<-1
Ensemble_probabilities$ensemble_plus_bert_probs_not<-1
Ensemble_probabilities$avg_preds_bert_plus_ensemble<-1
testing$support_system<-1
Ensemble_probabilities$bertpreds<-1
Ensemble_probabilities$bert_off<-Bert_results$`1`
Ensemble_probabilities$bert_not<-Bert_results$`0`


#Calculating average OFF prob and class prediction
#for (i in 1:nrow(Ensemble_probabilities)){
#  Ensemble_probabilities$ensemble_probs_off[i] <- (Ensemble_probabilities$SVM$.pred_OFF[i] + Ensemble_probabilities$NB$.pred_OFF[i] + Ensemble_probabilities$log$.pred_OFF[i]) / 3
#}

#Calculating average NOT prob
#for (i in 1:nrow(Ensemble_probabilities)){
#  testing$ensemble_probs_not[i] <- (Ensemble_probabilities$SVM$.pred_NOT[i] + Ensemble_probabilities$NB$.pred_NOT[i] + Ensemble_probabilities$log$.pred_NOT[i]) / 3
#  testing$avg_preds[i] <- ifelse(testing$ensemble_probs_off[i] > 0.5, 1, 0)
#  }


#Calculating average OFF BERT+Ensemble prob
for (i in 1:nrow(Ensemble_probabilities)){
  Ensemble_probabilities$ensemble_plus_bert_probs_off[i] <- (Ensemble_probabilities$SVM$.pred_OFF[i] + Ensemble_probabilities$NB$.pred_OFF[i] + Ensemble_probabilities$log$.pred_OFF[i] + Bert_results$`1`[i]) / 4
}

#Calculating average NOT BERT+Ensemble prob and making binary classification
for (i in 1:nrow(Ensemble_probabilities)){
  Ensemble_probabilities$ensemble_plus_bert_probs_not[i] <- (Ensemble_probabilities$SVM$.pred_NOT[i] + Ensemble_probabilities$NB$.pred_NOT[i] + Ensemble_probabilities$log$.pred_NOT[i] + Bert_results$`0`[i]) / 4
  Ensemble_probabilities$avg_preds_bert_plus_ensemble[i] <- ifelse(Ensemble_probabilities$ensemble_plus_bert_probs_off[i] > 0.5, 1, 0)
}

#Calculate binary BERT classification
for (i in 1:nrow(Ensemble_probabilities)){
  Ensemble_probabilities$bertpreds[i] <- ifelse(Ensemble_probabilities$bert_off[i] > 0.5, 1, 0)
}

##ENSEMBLE SUPPORT SYSTEM##

# Making support system integrating "unsure" offensive classifications (by BERT)
for (i in 1:nrow(testing)){
  testing$support_system[i]<-ifelse(Ensemble_probabilities$bert_off[i] > 0.5 & Ensemble_probabilities$bert_off[i] < 0.65, Ensemble_probabilities$avg_preds_bert_plus_ensemble[i], Ensemble_probabilities$bertpreds[i])
}

# Same but for NOT
for (i in 1:nrow(testing)){
  testing$support_system[i]<-ifelse(Ensemble_probabilities$bert_not[i] > 0.5 & Ensemble_probabilities$bert_not[i] < 0.65, Ensemble_probabilities$avg_preds_bert_plus_ensemble[i], Ensemble_probabilities$bertpreds[i])
}

(Ensemble_probabilities$SVM$.pred_NOT[151]+Ensemble_probabilities$NB$.pred_NOT[151]+Ensemble_probabilities$log$.pred_NOT[151])/3

(Ensemble_probabilities$SVM$.pred_OFF[151]+Ensemble_probabilities$NB$.pred_OFF[151]+Ensemble_probabilities$log$.pred_OFF[151])/3

```


##TEST only with NB (to see if 151 changes)

```{r}
#Load BERT data:
Bert_results<-read_csv("OG_BERT_RESULTS.csv")
#testing$BERT_not_prob<-Bert_results$`0`
#testing$BERT_off_prob<-Bert_results$`1`

#Create ensemble with raw probs of other models:
Ensemble_probabilities<-predictions_NB$raw_NB
Ensemble_probabilities$SVM<-predictions_SVM$raw_svm
Ensemble_probabilities<-Ensemble_probabilities[,3]
Ensemble_probabilities$NB<-predictions_NB$raw_NB
Ensemble_probabilities$log<-predictions_log$raw_log


#Define empty columns in testing set
Ensemble_probabilities$ensemble_plus_bert_probs_off<-1
Ensemble_probabilities$ensemble_plus_bert_probs_not<-1
Ensemble_probabilities$avg_preds_bert_plus_ensemble<-1
testing$support_system<-1
Ensemble_probabilities$bertpreds<-1
Ensemble_probabilities$bert_off<-Bert_results$`1`
Ensemble_probabilities$bert_not<-Bert_results$`0`


#Calculating average OFF prob and class prediction
#for (i in 1:nrow(Ensemble_probabilities)){
#  Ensemble_probabilities$ensemble_probs_off[i] <- (Ensemble_probabilities$SVM$.pred_OFF[i] + Ensemble_probabilities$NB$.pred_OFF[i] + Ensemble_probabilities$log$.pred_OFF[i]) / 3
#}

#Calculating average NOT prob
#for (i in 1:nrow(Ensemble_probabilities)){
#  testing$ensemble_probs_not[i] <- (Ensemble_probabilities$SVM$.pred_NOT[i] + Ensemble_probabilities$NB$.pred_NOT[i] + Ensemble_probabilities$log$.pred_NOT[i]) / 3
#  testing$avg_preds[i] <- ifelse(testing$ensemble_probs_off[i] > 0.5, 1, 0)
#  }


#Calculating average OFF BERT+Ensemble prob
for (i in 1:nrow(Ensemble_probabilities)){
  Ensemble_probabilities$ensemble_plus_bert_probs_off[i] <- (Ensemble_probabilities$NB$.pred_OFF[i] + Bert_results$`1`[i]) / 2
}

#Calculating average NOT BERT+Ensemble prob and making binary classification
for (i in 1:nrow(Ensemble_probabilities)){
  Ensemble_probabilities$ensemble_plus_bert_probs_not[i] <- (Ensemble_probabilities$NB$.pred_NOT[i] + Bert_results$`0`[i]) / 2
  Ensemble_probabilities$avg_preds_bert_plus_ensemble[i] <- ifelse(Ensemble_probabilities$ensemble_plus_bert_probs_off[i] > 0.5, 1, 0)
}

#Calculate binary BERT classification
for (i in 1:nrow(Ensemble_probabilities)){
  Ensemble_probabilities$bertpreds[i] <- ifelse(Ensemble_probabilities$bert_off[i] > 0.5, 1, 0)
}

##ENSEMBLE SUPPORT SYSTEM##

# Making support system integrating "unsure" offensive classifications (by BERT)
for (i in 1:nrow(testing)){
  testing$support_system[i]<-ifelse(Ensemble_probabilities$bert_off[i] > 0.5 & Ensemble_probabilities$bert_off[i] < 0.65, Ensemble_probabilities$avg_preds_bert_plus_ensemble[i], Ensemble_probabilities$bertpreds[i])
}

# Same but for NOT
for (i in 1:nrow(testing)){
  testing$support_system[i]<-ifelse(Ensemble_probabilities$bert_not[i] > 0.5 & Ensemble_probabilities$bert_not[i] < 0.65, Ensemble_probabilities$avg_preds_bert_plus_ensemble[i], Ensemble_probabilities$bertpreds[i])
}

(Ensemble_probabilities$SVM$.pred_NOT[151]+Ensemble_probabilities$NB$.pred_NOT[151]+Ensemble_probabilities$log$.pred_NOT[151])/3

(Ensemble_probabilities$SVM$.pred_OFF[151]+Ensemble_probabilities$NB$.pred_OFF[151]+Ensemble_probabilities$log$.pred_OFF[151])/3
```

